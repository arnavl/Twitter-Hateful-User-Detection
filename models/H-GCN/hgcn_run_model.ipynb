{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uditi\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils import *\n",
    "from models import GCN, MLP,HGCN\n",
    "from coarsen import *\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_string('dataset', 'hateful', 'Dataset string.')  # 'cora', 'citeseer', 'pubmed'\n",
    "flags.DEFINE_string('model', 'hgcn', 'Model string.')  # 'hgcn', 'gcn', 'gcn_cheby', 'dense'\n",
    "flags.DEFINE_float('learning_rate', 0.03, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 100, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('seed1', 123, 'random seed for numpy.')\n",
    "flags.DEFINE_integer('seed2', 123, 'random seed for tf.')\n",
    "flags.DEFINE_integer('hidden', 32, 'Number of units in hidden layer 1.')    \n",
    "flags.DEFINE_integer('node_wgt_embed_dim', 5, 'Number of units for node weight embedding.')   \n",
    "flags.DEFINE_float('dropout', 0.9, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 7e-4, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_integer('early_stopping', 1000, 'Tolerance for early stopping (# of epochs).')\n",
    "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')\n",
    "flags.DEFINE_integer('coarsen_level', 4, 'Maximum coarsen level.')\n",
    "flags.DEFINE_integer('max_node_wgt', 50, 'Maximum node_wgt to avoid super-node being too large.')\n",
    "flags.DEFINE_integer('channel_num', 4, 'Number of channels')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "seed1 = FLAGS.seed1\n",
    "seed2 = FLAGS.seed2\n",
    "np.random.seed(seed1)\n",
    "tf.set_random_seed(seed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(FLAGS.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total nodes: 7315\n"
     ]
    }
   ],
   "source": [
    "# Some preprocessing\n",
    "features = preprocess_features(features)\n",
    "if FLAGS.model == 'gcn': \n",
    "    support = [preprocess_adj(adj)]  # Not used\n",
    "    num_supports = 1\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'gcn_cheby':\n",
    "    support = chebyshev_polynomials(adj, FLAGS.max_degree)  # Not used\n",
    "    num_supports = 1 + FLAGS.max_degree\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'dense':\n",
    "    support = [preprocess_adj(adj)]  # Not used\n",
    "    num_supports = 1\n",
    "    model_func = MLP\n",
    "elif FLAGS.model == 'hgcn':\n",
    "    support = [preprocess_adj(adj)]  \n",
    "    num_supports = 1\n",
    "    model_func = HGCN    \n",
    "else:\n",
    "    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))\n",
    "\n",
    "graph, mapping = read_graph_from_adj(adj, FLAGS.dataset)\n",
    "print('total nodes:', graph.node_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5838 nodes in the 1 coarsened graph\n",
      "There are 4922 nodes in the 2 coarsened graph\n",
      "There are 4313 nodes in the 3 coarsened graph\n",
      "There are 3918 nodes in the 4 coarsened graph\n",
      "\n",
      "\n",
      "layer_index  1\n",
      "input shape:    (7315, 320)\n"
     ]
    }
   ],
   "source": [
    "# Step-1: Graph Coarsening.\n",
    "original_graph = graph\n",
    "transfer_list = []\n",
    "adj_list = [copy.copy(graph.A)]\n",
    "node_wgt_list = [copy.copy(graph.node_wgt)]\n",
    "for i in range(FLAGS.coarsen_level):\n",
    "    match, coarse_graph_size = generate_hybrid_matching(FLAGS.max_node_wgt, graph)\n",
    "    coarse_graph = create_coarse_graph(graph, match, coarse_graph_size)\n",
    "    transfer_list.append(copy.copy(graph.C))\n",
    "    graph = coarse_graph\n",
    "    adj_list.append(copy.copy(graph.A))  \n",
    "    node_wgt_list.append(copy.copy(graph.node_wgt))\n",
    "    print('There are %d nodes in the %d coarsened graph' %(graph.node_num, i+1))\n",
    "    \n",
    "print(\"\\n\")\n",
    "print('layer_index ', 1)\n",
    "print('input shape:   ', features[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape:     [5838, 32]\n",
      "layer_index  2\n",
      "input shape:    [5838, 32]\n",
      "output shape:     [4922, 32]\n",
      "layer_index  3\n",
      "input shape:    [4922, 32]\n",
      "output shape:     [4313, 32]\n",
      "layer_index  4\n",
      "input shape:    [4313, 32]\n",
      "output shape:     [3918, 32]\n",
      "layer_index  5\n",
      "input shape:    [3918, 32]\n",
      "output shape:     [4313, 32]\n",
      "layer_index  6\n",
      "input shape:    [4313, 32]\n",
      "output shape:     [4922, 32]\n",
      "layer_index  7\n",
      "input shape:    [4922, 32]\n",
      "output shape:     [5838, 32]\n",
      "layer_index  8\n",
      "input shape:    [5838, 32]\n",
      "output shape:     [7315, 32]\n",
      "layer_index  9\n",
      "input shape:    [7315, 32]\n",
      "output shape:     [7315, 3]\n",
      "WARNING:tensorflow:From C:\\Users\\uditi\\Desktop\\Final Year Project\\models\\H-GCN\\metrics.py:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(adj_list)):\n",
    "    adj_list[i] = [preprocess_adj(adj_list[i])]\n",
    "\n",
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
    "    'labels_mask': tf.placeholder(tf.int32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = model_func(placeholders, input_dim=features[2][1], logging=True, transfer_list = transfer_list, adj_list = adj_list, node_wgt_list = node_wgt_list)\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model evaluation function\n",
    "def evaluate(features, labels, mask, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(features, labels, mask, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)\n",
    "\n",
    "# Init variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "cost_val = []\n",
    "acc_val = []\n",
    "\n",
    "\n",
    "cost_train = []\n",
    "acc_train = []\n",
    "\n",
    "cost_test = []\n",
    "acc_test = []\n",
    "best_fcn = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 229.97504 train_acc= 0.32516 val_loss= 1.51765 val_acc= 0.46400 test_acc= 0.56967 time= 2.57894       best test_acc= 0.56967\n",
      "Epoch: 0002 train_loss= 52.73110 train_acc= 0.44756 val_loss= 1.55825 val_acc= 0.46200 test_acc= 0.57889 time= 1.87765       best test_acc= 0.57889\n",
      "Epoch: 0003 train_loss= 10.09617 train_acc= 0.49862 val_loss= 1.60707 val_acc= 0.46600 test_acc= 0.58197 time= 1.83154       best test_acc= 0.58197\n",
      "Epoch: 0004 train_loss= 11.63850 train_acc= 0.53116 val_loss= 1.66016 val_acc= 0.46400 test_acc= 0.58504 time= 1.71295       best test_acc= 0.58504\n",
      "Epoch: 0005 train_loss= 5.92467 train_acc= 0.52941 val_loss= 1.71398 val_acc= 0.46400 test_acc= 0.58607 time= 1.72685       best test_acc= 0.58607\n",
      "Epoch: 0006 train_loss= 5.69362 train_acc= 0.53442 val_loss= 1.76373 val_acc= 0.46600 test_acc= 0.58709 time= 1.68987       best test_acc= 0.58709\n",
      "Epoch: 0007 train_loss= 4.66948 train_acc= 0.52966 val_loss= 1.81008 val_acc= 0.47200 test_acc= 0.58607 time= 1.71383       best test_acc= 0.58709\n",
      "Epoch: 0008 train_loss= 4.78408 train_acc= 0.53016 val_loss= 1.84995 val_acc= 0.47800 test_acc= 0.58197 time= 1.70234       best test_acc= 0.58709\n",
      "Epoch: 0009 train_loss= 3.78848 train_acc= 0.52265 val_loss= 1.88550 val_acc= 0.47000 test_acc= 0.57992 time= 1.71359       best test_acc= 0.58709\n",
      "Epoch: 0010 train_loss= 4.08404 train_acc= 0.51189 val_loss= 1.91720 val_acc= 0.48000 test_acc= 0.57275 time= 1.71693       best test_acc= 0.58709\n",
      "Epoch: 0011 train_loss= 4.87754 train_acc= 0.50163 val_loss= 1.94541 val_acc= 0.49000 test_acc= 0.57172 time= 1.73045       best test_acc= 0.58709\n",
      "Epoch: 0012 train_loss= 4.72147 train_acc= 0.50438 val_loss= 1.96966 val_acc= 0.49600 test_acc= 0.55738 time= 1.71673       best test_acc= 0.58709\n",
      "Epoch: 0013 train_loss= 4.49975 train_acc= 0.49587 val_loss= 1.99039 val_acc= 0.49200 test_acc= 0.54816 time= 1.71688       best test_acc= 0.58709\n",
      "Epoch: 0014 train_loss= 5.43158 train_acc= 0.47409 val_loss= 2.00900 val_acc= 0.49200 test_acc= 0.53586 time= 1.68717       best test_acc= 0.58709\n",
      "Epoch: 0015 train_loss= 5.11159 train_acc= 0.46808 val_loss= 2.02383 val_acc= 0.49800 test_acc= 0.52766 time= 1.71378       best test_acc= 0.58709\n",
      "Epoch: 0016 train_loss= 7.82178 train_acc= 0.48360 val_loss= 2.03474 val_acc= 0.49000 test_acc= 0.50615 time= 1.71470       best test_acc= 0.58709\n",
      "Epoch: 0017 train_loss= 3.75913 train_acc= 0.48010 val_loss= 2.04384 val_acc= 0.49200 test_acc= 0.49795 time= 1.69946       best test_acc= 0.58709\n",
      "Epoch: 0018 train_loss= 3.74501 train_acc= 0.46883 val_loss= 2.05228 val_acc= 0.48800 test_acc= 0.49283 time= 1.73191       best test_acc= 0.58709\n",
      "Epoch: 0019 train_loss= 4.43858 train_acc= 0.47484 val_loss= 2.05935 val_acc= 0.49200 test_acc= 0.48873 time= 1.78017       best test_acc= 0.58709\n",
      "Epoch: 0020 train_loss= 5.22673 train_acc= 0.47935 val_loss= 2.06546 val_acc= 0.49000 test_acc= 0.48770 time= 1.78287       best test_acc= 0.58709\n",
      "Epoch: 0021 train_loss= 3.52105 train_acc= 0.47835 val_loss= 2.07115 val_acc= 0.49000 test_acc= 0.49795 time= 1.78543       best test_acc= 0.58709\n",
      "Epoch: 0022 train_loss= 3.41845 train_acc= 0.47459 val_loss= 2.07672 val_acc= 0.48600 test_acc= 0.49078 time= 1.78220       best test_acc= 0.58709\n",
      "Epoch: 0023 train_loss= 4.52423 train_acc= 0.47034 val_loss= 2.08222 val_acc= 0.49200 test_acc= 0.49385 time= 1.76775       best test_acc= 0.58709\n",
      "Epoch: 0024 train_loss= 4.89460 train_acc= 0.46959 val_loss= 2.08673 val_acc= 0.49000 test_acc= 0.49795 time= 1.78225       best test_acc= 0.58709\n",
      "Epoch: 0025 train_loss= 4.82459 train_acc= 0.48636 val_loss= 2.09137 val_acc= 0.48400 test_acc= 0.51229 time= 1.78067       best test_acc= 0.58709\n",
      "Epoch: 0026 train_loss= 4.19033 train_acc= 0.47459 val_loss= 2.09667 val_acc= 0.48200 test_acc= 0.52664 time= 1.78542       best test_acc= 0.58709\n",
      "Epoch: 0027 train_loss= 3.92819 train_acc= 0.49962 val_loss= 2.10183 val_acc= 0.46600 test_acc= 0.52357 time= 1.78241       best test_acc= 0.58709\n",
      "Epoch: 0028 train_loss= 3.83763 train_acc= 0.49236 val_loss= 2.10599 val_acc= 0.45800 test_acc= 0.52664 time= 1.80517       best test_acc= 0.58709\n",
      "Epoch: 0029 train_loss= 3.28506 train_acc= 0.48385 val_loss= 2.10928 val_acc= 0.46200 test_acc= 0.52766 time= 2.05379       best test_acc= 0.58709\n",
      "Epoch: 0030 train_loss= 3.50370 train_acc= 0.49412 val_loss= 2.11121 val_acc= 0.45800 test_acc= 0.53074 time= 1.86102       best test_acc= 0.58709\n",
      "Epoch: 0031 train_loss= 4.40095 train_acc= 0.49412 val_loss= 2.11196 val_acc= 0.44800 test_acc= 0.53176 time= 1.86313       best test_acc= 0.58709\n",
      "Epoch: 0032 train_loss= 3.89007 train_acc= 0.50388 val_loss= 2.11135 val_acc= 0.45200 test_acc= 0.53176 time= 1.92293       best test_acc= 0.58709\n",
      "Epoch: 0033 train_loss= 3.73646 train_acc= 0.48886 val_loss= 2.11073 val_acc= 0.45400 test_acc= 0.53996 time= 1.87842       best test_acc= 0.58709\n",
      "Epoch: 0034 train_loss= 3.84513 train_acc= 0.48561 val_loss= 2.10910 val_acc= 0.45800 test_acc= 0.54201 time= 1.77777       best test_acc= 0.58709\n",
      "Epoch: 0035 train_loss= 3.59898 train_acc= 0.49537 val_loss= 2.10645 val_acc= 0.46000 test_acc= 0.53996 time= 1.76633       best test_acc= 0.58709\n",
      "Epoch: 0036 train_loss= 3.20785 train_acc= 0.47935 val_loss= 2.10385 val_acc= 0.46000 test_acc= 0.53484 time= 1.78399       best test_acc= 0.58709\n",
      "Epoch: 0037 train_loss= 3.72346 train_acc= 0.48235 val_loss= 2.10108 val_acc= 0.46600 test_acc= 0.54098 time= 1.78312       best test_acc= 0.58709\n",
      "Epoch: 0038 train_loss= 2.92317 train_acc= 0.48235 val_loss= 2.09830 val_acc= 0.46600 test_acc= 0.54201 time= 1.76685       best test_acc= 0.58709\n",
      "Epoch: 0039 train_loss= 3.24439 train_acc= 0.48335 val_loss= 2.09556 val_acc= 0.46800 test_acc= 0.54303 time= 1.76629       best test_acc= 0.58709\n",
      "Epoch: 0040 train_loss= 3.56092 train_acc= 0.49437 val_loss= 2.09288 val_acc= 0.46600 test_acc= 0.54611 time= 1.78290       best test_acc= 0.58709\n",
      "Epoch: 0041 train_loss= 3.63357 train_acc= 0.49186 val_loss= 2.08958 val_acc= 0.46200 test_acc= 0.54303 time= 1.76645       best test_acc= 0.58709\n",
      "Epoch: 0042 train_loss= 2.75949 train_acc= 0.48510 val_loss= 2.08618 val_acc= 0.45800 test_acc= 0.53893 time= 1.78244       best test_acc= 0.58709\n",
      "Epoch: 0043 train_loss= 3.18070 train_acc= 0.47459 val_loss= 2.08210 val_acc= 0.45800 test_acc= 0.53586 time= 1.76516       best test_acc= 0.58709\n",
      "Epoch: 0044 train_loss= 3.06414 train_acc= 0.48235 val_loss= 2.07766 val_acc= 0.46200 test_acc= 0.53893 time= 1.76630       best test_acc= 0.58709\n",
      "Epoch: 0045 train_loss= 2.85127 train_acc= 0.49687 val_loss= 2.07244 val_acc= 0.46200 test_acc= 0.53689 time= 1.84610       best test_acc= 0.58709\n",
      "Epoch: 0046 train_loss= 2.93611 train_acc= 0.46959 val_loss= 2.06699 val_acc= 0.46600 test_acc= 0.54098 time= 1.95736       best test_acc= 0.58709\n",
      "Epoch: 0047 train_loss= 2.62995 train_acc= 0.48210 val_loss= 2.06120 val_acc= 0.46800 test_acc= 0.53996 time= 1.82333       best test_acc= 0.58709\n",
      "Epoch: 0048 train_loss= 3.08060 train_acc= 0.48485 val_loss= 2.05435 val_acc= 0.46600 test_acc= 0.54406 time= 1.77187       best test_acc= 0.58709\n",
      "Epoch: 0049 train_loss= 3.19920 train_acc= 0.46558 val_loss= 2.04695 val_acc= 0.46400 test_acc= 0.54098 time= 1.76828       best test_acc= 0.58709\n",
      "Epoch: 0050 train_loss= 2.64877 train_acc= 0.48786 val_loss= 2.03932 val_acc= 0.46200 test_acc= 0.53381 time= 1.77474       best test_acc= 0.58709\n",
      "Epoch: 0051 train_loss= 2.48320 train_acc= 0.47509 val_loss= 2.03179 val_acc= 0.45600 test_acc= 0.52561 time= 1.77250       best test_acc= 0.58709\n",
      "Epoch: 0052 train_loss= 2.58839 train_acc= 0.45782 val_loss= 2.02481 val_acc= 0.46600 test_acc= 0.52459 time= 1.78362       best test_acc= 0.58709\n",
      "Epoch: 0053 train_loss= 2.42802 train_acc= 0.46533 val_loss= 2.01788 val_acc= 0.46000 test_acc= 0.52459 time= 1.76615       best test_acc= 0.58709\n",
      "Epoch: 0054 train_loss= 2.43912 train_acc= 0.47960 val_loss= 2.01103 val_acc= 0.45600 test_acc= 0.53074 time= 1.76753       best test_acc= 0.58709\n",
      "Epoch: 0055 train_loss= 2.36098 train_acc= 0.47234 val_loss= 2.00500 val_acc= 0.45400 test_acc= 0.54098 time= 1.76630       best test_acc= 0.58709\n",
      "Epoch: 0056 train_loss= 2.46621 train_acc= 0.48260 val_loss= 1.99988 val_acc= 0.46600 test_acc= 0.55123 time= 1.81393       best test_acc= 0.58709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0057 train_loss= 2.84191 train_acc= 0.48185 val_loss= 1.99621 val_acc= 0.46400 test_acc= 0.56250 time= 1.76879       best test_acc= 0.58709\n",
      "Epoch: 0058 train_loss= 2.51554 train_acc= 0.49987 val_loss= 1.99295 val_acc= 0.46800 test_acc= 0.57684 time= 1.78024       best test_acc= 0.58709\n",
      "Epoch: 0059 train_loss= 2.35688 train_acc= 0.50563 val_loss= 1.99045 val_acc= 0.47200 test_acc= 0.57684 time= 1.76740       best test_acc= 0.58709\n",
      "Epoch: 0060 train_loss= 2.64171 train_acc= 0.50288 val_loss= 1.98897 val_acc= 0.47000 test_acc= 0.58197 time= 1.79755       best test_acc= 0.58709\n",
      "Epoch: 0061 train_loss= 2.39496 train_acc= 0.49161 val_loss= 1.98740 val_acc= 0.46400 test_acc= 0.58402 time= 1.76836       best test_acc= 0.58709\n",
      "Epoch: 0062 train_loss= 2.45687 train_acc= 0.50813 val_loss= 1.98661 val_acc= 0.46400 test_acc= 0.58402 time= 1.83008       best test_acc= 0.58709\n",
      "Epoch: 0063 train_loss= 2.21793 train_acc= 0.50888 val_loss= 1.98578 val_acc= 0.46600 test_acc= 0.58402 time= 1.80896       best test_acc= 0.58709\n",
      "Epoch: 0064 train_loss= 2.37283 train_acc= 0.51439 val_loss= 1.98476 val_acc= 0.47000 test_acc= 0.58402 time= 1.76927       best test_acc= 0.58709\n",
      "Epoch: 0065 train_loss= 2.40143 train_acc= 0.50838 val_loss= 1.98362 val_acc= 0.47000 test_acc= 0.58504 time= 1.86587       best test_acc= 0.58709\n",
      "Epoch: 0066 train_loss= 2.23445 train_acc= 0.51689 val_loss= 1.98184 val_acc= 0.47000 test_acc= 0.58504 time= 1.85441       best test_acc= 0.58709\n",
      "Epoch: 0067 train_loss= 2.15966 train_acc= 0.52891 val_loss= 1.97989 val_acc= 0.47000 test_acc= 0.58504 time= 1.76942       best test_acc= 0.58709\n",
      "Epoch: 0068 train_loss= 2.22165 train_acc= 0.52540 val_loss= 1.97779 val_acc= 0.46400 test_acc= 0.58504 time= 1.76779       best test_acc= 0.58709\n",
      "Epoch: 0069 train_loss= 2.58227 train_acc= 0.52966 val_loss= 1.97551 val_acc= 0.46200 test_acc= 0.58299 time= 1.78198       best test_acc= 0.58709\n",
      "Epoch: 0070 train_loss= 2.20099 train_acc= 0.52866 val_loss= 1.97245 val_acc= 0.46200 test_acc= 0.58299 time= 1.76757       best test_acc= 0.58709\n",
      "Epoch: 0071 train_loss= 2.24622 train_acc= 0.52290 val_loss= 1.96935 val_acc= 0.46400 test_acc= 0.58402 time= 1.76693       best test_acc= 0.58709\n",
      "Epoch: 0072 train_loss= 2.16668 train_acc= 0.53617 val_loss= 1.96553 val_acc= 0.46400 test_acc= 0.58197 time= 1.76547       best test_acc= 0.58709\n",
      "Epoch: 0073 train_loss= 2.10240 train_acc= 0.52741 val_loss= 1.96168 val_acc= 0.46400 test_acc= 0.58197 time= 1.76469       best test_acc= 0.58709\n",
      "Epoch: 0074 train_loss= 2.08812 train_acc= 0.52415 val_loss= 1.95799 val_acc= 0.46400 test_acc= 0.58299 time= 1.76806       best test_acc= 0.58709\n",
      "Epoch: 0075 train_loss= 2.15114 train_acc= 0.52240 val_loss= 1.95433 val_acc= 0.46000 test_acc= 0.58299 time= 1.78236       best test_acc= 0.58709\n",
      "Epoch: 0076 train_loss= 2.07156 train_acc= 0.53492 val_loss= 1.95059 val_acc= 0.46600 test_acc= 0.58504 time= 1.77830       best test_acc= 0.58709\n",
      "Epoch: 0077 train_loss= 2.02951 train_acc= 0.52866 val_loss= 1.94664 val_acc= 0.46600 test_acc= 0.58811 time= 1.75430       best test_acc= 0.58811\n",
      "Epoch: 0078 train_loss= 2.19592 train_acc= 0.51689 val_loss= 1.94217 val_acc= 0.46400 test_acc= 0.58811 time= 1.76694       best test_acc= 0.58811\n",
      "Epoch: 0079 train_loss= 2.17986 train_acc= 0.52741 val_loss= 1.93812 val_acc= 0.46400 test_acc= 0.58811 time= 1.79755       best test_acc= 0.58811\n",
      "Epoch: 0080 train_loss= 2.04546 train_acc= 0.52966 val_loss= 1.93451 val_acc= 0.46400 test_acc= 0.58709 time= 1.76672       best test_acc= 0.58811\n",
      "Epoch: 0081 train_loss= 2.05222 train_acc= 0.53191 val_loss= 1.93102 val_acc= 0.46400 test_acc= 0.58709 time= 1.76858       best test_acc= 0.58811\n",
      "Epoch: 0082 train_loss= 2.08833 train_acc= 0.53892 val_loss= 1.92765 val_acc= 0.46600 test_acc= 0.58811 time= 1.78171       best test_acc= 0.58811\n",
      "Epoch: 0083 train_loss= 2.28407 train_acc= 0.52841 val_loss= 1.92446 val_acc= 0.46600 test_acc= 0.58914 time= 1.76718       best test_acc= 0.58914\n",
      "Epoch: 0084 train_loss= 2.00593 train_acc= 0.54718 val_loss= 1.92111 val_acc= 0.46800 test_acc= 0.58914 time= 1.77432       best test_acc= 0.58914\n",
      "Epoch: 0085 train_loss= 2.01569 train_acc= 0.53792 val_loss= 1.91782 val_acc= 0.46800 test_acc= 0.58914 time= 1.80903       best test_acc= 0.58914\n",
      "Epoch: 0086 train_loss= 2.09362 train_acc= 0.53066 val_loss= 1.91486 val_acc= 0.46800 test_acc= 0.58914 time= 2.46050       best test_acc= 0.58914\n",
      "Epoch: 0087 train_loss= 2.12994 train_acc= 0.54142 val_loss= 1.91187 val_acc= 0.46800 test_acc= 0.58811 time= 1.92261       best test_acc= 0.58914\n",
      "Epoch: 0088 train_loss= 2.10224 train_acc= 0.54618 val_loss= 1.90888 val_acc= 0.46800 test_acc= 0.58811 time= 2.01698       best test_acc= 0.58914\n",
      "Epoch: 0089 train_loss= 2.01952 train_acc= 0.54818 val_loss= 1.90643 val_acc= 0.46800 test_acc= 0.58811 time= 2.01967       best test_acc= 0.58914\n",
      "Epoch: 0090 train_loss= 2.20798 train_acc= 0.53842 val_loss= 1.90403 val_acc= 0.46600 test_acc= 0.58607 time= 1.96125       best test_acc= 0.58914\n",
      "Epoch: 0091 train_loss= 2.05828 train_acc= 0.54193 val_loss= 1.90145 val_acc= 0.46400 test_acc= 0.58504 time= 1.79680       best test_acc= 0.58914\n",
      "Epoch: 0092 train_loss= 2.08874 train_acc= 0.52140 val_loss= 1.89869 val_acc= 0.46200 test_acc= 0.58402 time= 1.75805       best test_acc= 0.58914\n",
      "Epoch: 0093 train_loss= 2.30793 train_acc= 0.53141 val_loss= 1.89638 val_acc= 0.46200 test_acc= 0.58197 time= 1.79425       best test_acc= 0.58914\n",
      "Epoch: 0094 train_loss= 2.26102 train_acc= 0.51815 val_loss= 1.89389 val_acc= 0.46400 test_acc= 0.58402 time= 1.76395       best test_acc= 0.58914\n",
      "Epoch: 0095 train_loss= 1.95591 train_acc= 0.53266 val_loss= 1.89148 val_acc= 0.46800 test_acc= 0.58607 time= 1.76730       best test_acc= 0.58914\n",
      "Epoch: 0096 train_loss= 2.05863 train_acc= 0.52540 val_loss= 1.88924 val_acc= 0.46800 test_acc= 0.58607 time= 1.76651       best test_acc= 0.58914\n",
      "Epoch: 0097 train_loss= 2.04592 train_acc= 0.52966 val_loss= 1.88728 val_acc= 0.46800 test_acc= 0.58811 time= 1.76624       best test_acc= 0.58914\n",
      "Epoch: 0098 train_loss= 2.06024 train_acc= 0.54493 val_loss= 1.88503 val_acc= 0.46800 test_acc= 0.58914 time= 1.93833       best test_acc= 0.58914\n",
      "Epoch: 0099 train_loss= 2.68327 train_acc= 0.54318 val_loss= 1.88248 val_acc= 0.46800 test_acc= 0.59016 time= 1.79106       best test_acc= 0.59016\n",
      "Epoch: 0100 train_loss= 2.14972 train_acc= 0.55469 val_loss= 1.88018 val_acc= 0.46800 test_acc= 0.58914 time= 1.76976       best test_acc= 0.59016\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "for epoch in range(FLAGS.epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(features,  y_train, train_mask, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "\n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "    # Validation\n",
    "    cost, acc, duration = evaluate(features, y_val, val_mask, placeholders)\n",
    "    cost_val.append(cost)\n",
    "    \n",
    "    # Test\n",
    "    test_cost, test_acc, test_duration = evaluate(features, y_test, test_mask, placeholders)\n",
    "    cost_train.append(outs[1])\n",
    "    acc_train.append(outs[2])    \n",
    "    cost_test.append(test_cost)\n",
    "    acc_test.append(test_acc)\n",
    "    acc_val.append(acc)\n",
    "    if test_acc > best_fcn:\n",
    "        best_fcn = test_acc\n",
    "\n",
    "    # Print results\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "          \"val_acc=\", \"{:.5f}\".format(acc), \"test_acc=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(time.time() - t),\"      best test_acc=\", \"{:.5f}\".format(best_fcn),)\n",
    "\n",
    "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best epoch:    14\n",
      "test result:   0.5276637\n"
     ]
    }
   ],
   "source": [
    "############################### test acc for every epoch\n",
    "mat = np.array(acc_test)\n",
    "# print(np.max(mat))\n",
    "val_index_best =  np.argmax(np.array(acc_val))\n",
    "print('best epoch:   ',val_index_best)\n",
    "print('test result:  ',mat[val_index_best])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
