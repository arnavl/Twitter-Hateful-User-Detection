{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uditi\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils import *\n",
    "from models import GCN, MLP\n",
    "import easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Settings\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_string('dataset', 'hateful', 'Dataset string.')  # 'cora', 'citeseer', 'pubmed'\n",
    "flags.DEFINE_string('model', 'gcn', 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 20, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_integer('early_stopping', 10, 'Tolerance for early stopping (# of epochs).')\n",
    "flags.DEFINE_integer('max_degree', 2, 'Maximum Chebyshev polynomial degree.')\n",
    "\n",
    "\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "        \"dataset\": \"hateful\",\n",
    "        \"model\": \"gcn\",\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"hidden1\": 20,\n",
    "        \"epochs\" : 200,\n",
    "        \"dropout\": 0.5,\n",
    "        \"weight_decay\": 5e-4,\n",
    "        \"early_stopping\": 10,\n",
    "        \"max_degree\": 2\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(FLAGS.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = preprocess_features(features)\n",
    "if FLAGS.model == 'gcn':\n",
    "    support = [preprocess_adj(adj)]\n",
    "    num_supports = 1\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'gcn_cheby':\n",
    "    support = chebyshev_polynomials(adj, FLAGS.max_degree)\n",
    "    num_supports = 1 + FLAGS.max_degree\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'dense':\n",
    "    support = [preprocess_adj(adj)]  # Not used\n",
    "    num_supports = 1\n",
    "    model_func = MLP\n",
    "else:\n",
    "    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
    "    'labels_mask': tf.placeholder(tf.int32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\uditi\\Desktop\\Final Year Project\\models\\gcn\\metrics.py:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = model_func(placeholders, input_dim=features[2][1], logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize session\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, mask, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "cost_val = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.11698 train_acc= 0.53838 val_loss= 1.12915 val_acc= 0.47200 time= 3.73969\n",
      "Epoch: 0002 train_loss= 1.09062 train_acc= 0.56064 val_loss= 1.10609 val_acc= 0.47000 time= 2.61132\n",
      "Epoch: 0003 train_loss= 1.04536 train_acc= 0.61240 val_loss= 1.09743 val_acc= 0.46600 time= 2.47837\n",
      "Epoch: 0004 train_loss= 1.01798 train_acc= 0.60415 val_loss= 1.09164 val_acc= 0.46600 time= 2.66786\n",
      "Epoch: 0005 train_loss= 0.99234 train_acc= 0.62140 val_loss= 1.08585 val_acc= 0.46600 time= 2.57384\n",
      "Epoch: 0006 train_loss= 0.98381 train_acc= 0.61590 val_loss= 1.07929 val_acc= 0.46800 time= 2.44555\n",
      "Epoch: 0007 train_loss= 0.96409 train_acc= 0.62190 val_loss= 1.08004 val_acc= 0.47600 time= 2.43465\n",
      "Epoch: 0008 train_loss= 0.95146 train_acc= 0.62190 val_loss= 1.08196 val_acc= 0.47400 time= 2.41905\n",
      "Epoch: 0009 train_loss= 0.94613 train_acc= 0.62115 val_loss= 1.08412 val_acc= 0.47800 time= 2.42048\n",
      "Epoch: 0010 train_loss= 0.95170 train_acc= 0.61790 val_loss= 1.08830 val_acc= 0.47400 time= 2.42772\n",
      "Epoch: 0011 train_loss= 0.98672 train_acc= 0.61890 val_loss= 1.08643 val_acc= 0.47400 time= 2.41958\n",
      "Epoch: 0012 train_loss= 0.93834 train_acc= 0.61865 val_loss= 1.08409 val_acc= 0.47200 time= 2.42053\n",
      "Epoch: 0013 train_loss= 0.96686 train_acc= 0.62215 val_loss= 1.07917 val_acc= 0.47400 time= 2.46998\n",
      "Epoch: 0014 train_loss= 0.92419 train_acc= 0.62390 val_loss= 1.07304 val_acc= 0.47200 time= 2.42405\n",
      "Epoch: 0015 train_loss= 0.92983 train_acc= 0.62390 val_loss= 1.06767 val_acc= 0.47400 time= 2.41959\n",
      "Epoch: 0016 train_loss= 0.92965 train_acc= 0.62140 val_loss= 1.05832 val_acc= 0.47600 time= 2.42530\n",
      "Epoch: 0017 train_loss= 0.93835 train_acc= 0.61790 val_loss= 1.04967 val_acc= 0.47200 time= 2.42452\n",
      "Epoch: 0018 train_loss= 0.92049 train_acc= 0.62440 val_loss= 1.04388 val_acc= 0.47200 time= 2.44497\n",
      "Epoch: 0019 train_loss= 0.92728 train_acc= 0.61815 val_loss= 1.03672 val_acc= 0.47000 time= 2.42898\n",
      "Epoch: 0020 train_loss= 0.91516 train_acc= 0.62415 val_loss= 1.03216 val_acc= 0.47400 time= 2.43299\n",
      "Epoch: 0021 train_loss= 0.91702 train_acc= 0.61690 val_loss= 1.02902 val_acc= 0.47600 time= 2.53503\n",
      "Epoch: 0022 train_loss= 0.92096 train_acc= 0.61815 val_loss= 1.02656 val_acc= 0.47200 time= 2.47502\n",
      "Epoch: 0023 train_loss= 0.91890 train_acc= 0.61890 val_loss= 1.02478 val_acc= 0.47200 time= 2.42252\n",
      "Epoch: 0024 train_loss= 0.91720 train_acc= 0.62015 val_loss= 1.02296 val_acc= 0.47200 time= 2.42555\n",
      "Epoch: 0025 train_loss= 0.90748 train_acc= 0.62065 val_loss= 1.02290 val_acc= 0.47200 time= 2.44112\n",
      "Epoch: 0026 train_loss= 0.91398 train_acc= 0.62165 val_loss= 1.02249 val_acc= 0.47400 time= 2.43250\n",
      "Epoch: 0027 train_loss= 0.91200 train_acc= 0.62490 val_loss= 1.02002 val_acc= 0.47400 time= 2.45388\n",
      "Epoch: 0028 train_loss= 0.90805 train_acc= 0.62415 val_loss= 1.01880 val_acc= 0.47600 time= 2.42252\n",
      "Epoch: 0029 train_loss= 0.92010 train_acc= 0.62540 val_loss= 1.01500 val_acc= 0.47800 time= 2.41605\n",
      "Epoch: 0030 train_loss= 0.91346 train_acc= 0.62766 val_loss= 1.00964 val_acc= 0.48000 time= 2.43103\n",
      "Epoch: 0031 train_loss= 0.92236 train_acc= 0.62415 val_loss= 1.00329 val_acc= 0.48000 time= 2.42352\n",
      "Epoch: 0032 train_loss= 0.90277 train_acc= 0.62415 val_loss= 0.99777 val_acc= 0.48200 time= 2.43054\n",
      "Epoch: 0033 train_loss= 0.90369 train_acc= 0.62215 val_loss= 0.99461 val_acc= 0.48200 time= 2.49256\n",
      "Epoch: 0034 train_loss= 0.90686 train_acc= 0.62515 val_loss= 0.99283 val_acc= 0.48200 time= 2.46136\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "for epoch in range(FLAGS.epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(features, support, y_train, train_mask, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "\n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "    # Validation\n",
    "    cost, acc, duration = evaluate(features, support, y_val, val_mask, placeholders)\n",
    "    cost_val.append(cost)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "test_cost, test_acc, test_duration = evaluate(features, support, y_test, test_mask, placeholders)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
