{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from utils import *\n",
    "from models import GCN, MLP\n",
    "import easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Settings\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_string('dataset', 'hateful', 'Dataset string.')  # 'cora', 'citeseer', 'pubmed'\n",
    "flags.DEFINE_string('model', 'gcn', 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 20, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_integer('early_stopping', 10, 'Tolerance for early stopping (# of epochs).')\n",
    "flags.DEFINE_integer('max_degree', 2, 'Maximum Chebyshev polynomial degree.')\n",
    "\n",
    "\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "        \"dataset\": \"hateful\",\n",
    "        \"model\": \"gcn\",\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"hidden1\": 20,\n",
    "        \"epochs\" : 200,\n",
    "        \"dropout\": 0.5,\n",
    "        \"weight_decay\": 5e-4,\n",
    "        \"early_stopping\": 10,\n",
    "        \"max_degree\": 2\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(FLAGS.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = preprocess_features(features)\n",
    "if FLAGS.model == 'gcn':\n",
    "    support = [preprocess_adj(adj)]\n",
    "    num_supports = 1\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'gcn_cheby':\n",
    "    support = chebyshev_polynomials(adj, FLAGS.max_degree)\n",
    "    num_supports = 1 + FLAGS.max_degree\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'dense':\n",
    "    support = [preprocess_adj(adj)]  # Not used\n",
    "    num_supports = 1\n",
    "    model_func = MLP\n",
    "else:\n",
    "    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
    "    'labels_mask': tf.placeholder(tf.int32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\uditi\\Desktop\\Final Year Project\\models\\gcn\\metrics.py:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = model_func(placeholders, input_dim=features[2][1], logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize session\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, mask, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "cost_val = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.12286 train_acc= 0.51464 val_loss= 1.08760 val_acc= 0.45200 time= 3.30514\n",
      "Epoch: 0002 train_loss= 1.23645 train_acc= 0.58898 val_loss= 1.07177 val_acc= 0.45800 time= 2.42036\n",
      "Epoch: 0003 train_loss= 1.13121 train_acc= 0.59950 val_loss= 1.05773 val_acc= 0.46200 time= 2.63296\n",
      "Epoch: 0004 train_loss= 1.21780 train_acc= 0.60100 val_loss= 1.04237 val_acc= 0.46000 time= 2.53329\n",
      "Epoch: 0005 train_loss= 1.09237 train_acc= 0.61051 val_loss= 1.02620 val_acc= 0.45800 time= 2.58085\n",
      "Epoch: 0006 train_loss= 1.11170 train_acc= 0.60125 val_loss= 1.01272 val_acc= 0.46000 time= 2.54851\n",
      "Epoch: 0007 train_loss= 1.00389 train_acc= 0.60075 val_loss= 1.00185 val_acc= 0.45800 time= 2.52544\n",
      "Epoch: 0008 train_loss= 0.99915 train_acc= 0.60500 val_loss= 0.99241 val_acc= 0.46200 time= 2.51229\n",
      "Epoch: 0009 train_loss= 0.98976 train_acc= 0.59574 val_loss= 0.98636 val_acc= 0.46000 time= 2.51819\n",
      "Epoch: 0010 train_loss= 0.99151 train_acc= 0.60250 val_loss= 0.98191 val_acc= 0.46000 time= 2.52904\n",
      "Epoch: 0011 train_loss= 0.99958 train_acc= 0.60425 val_loss= 0.97755 val_acc= 0.46400 time= 2.53402\n",
      "Epoch: 0012 train_loss= 0.98967 train_acc= 0.60801 val_loss= 0.97466 val_acc= 0.46600 time= 2.55146\n",
      "Epoch: 0013 train_loss= 0.97197 train_acc= 0.60651 val_loss= 0.97235 val_acc= 0.47000 time= 2.52343\n",
      "Epoch: 0014 train_loss= 1.03833 train_acc= 0.61251 val_loss= 0.96844 val_acc= 0.47000 time= 2.52143\n",
      "Epoch: 0015 train_loss= 0.96592 train_acc= 0.60976 val_loss= 0.96415 val_acc= 0.47000 time= 2.86782\n",
      "Epoch: 0016 train_loss= 0.97271 train_acc= 0.60926 val_loss= 0.95892 val_acc= 0.47000 time= 2.51933\n",
      "Epoch: 0017 train_loss= 1.01124 train_acc= 0.60951 val_loss= 0.95367 val_acc= 0.46800 time= 2.55008\n",
      "Epoch: 0018 train_loss= 0.96349 train_acc= 0.61577 val_loss= 0.94941 val_acc= 0.46600 time= 2.52454\n",
      "Epoch: 0019 train_loss= 0.99650 train_acc= 0.60901 val_loss= 0.94467 val_acc= 0.46400 time= 2.63069\n",
      "Epoch: 0020 train_loss= 0.95297 train_acc= 0.61051 val_loss= 0.94074 val_acc= 0.46400 time= 2.52797\n",
      "Epoch: 0021 train_loss= 0.95805 train_acc= 0.60676 val_loss= 0.93803 val_acc= 0.46200 time= 2.86710\n",
      "Epoch: 0022 train_loss= 0.95112 train_acc= 0.60951 val_loss= 0.93634 val_acc= 0.46000 time= 2.75660\n",
      "Epoch: 0023 train_loss= 0.94795 train_acc= 0.60776 val_loss= 0.93483 val_acc= 0.46400 time= 2.60159\n",
      "Epoch: 0024 train_loss= 0.96656 train_acc= 0.60250 val_loss= 0.93405 val_acc= 0.46400 time= 2.52925\n",
      "Epoch: 0025 train_loss= 0.96903 train_acc= 0.61301 val_loss= 0.93376 val_acc= 0.46400 time= 2.54168\n",
      "Epoch: 0026 train_loss= 0.95688 train_acc= 0.59875 val_loss= 0.93381 val_acc= 0.46600 time= 2.67016\n",
      "Epoch: 0027 train_loss= 0.94568 train_acc= 0.60676 val_loss= 0.93436 val_acc= 0.46200 time= 2.52399\n",
      "Epoch: 0028 train_loss= 0.97765 train_acc= 0.60225 val_loss= 0.93541 val_acc= 0.46400 time= 2.55341\n",
      "Epoch: 0029 train_loss= 0.94714 train_acc= 0.61051 val_loss= 0.93682 val_acc= 0.46200 time= 2.54945\n",
      "Early stopping...\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "for epoch in range(FLAGS.epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(features, support, y_train, train_mask, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "\n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "    # Validation\n",
    "    cost, acc, duration = evaluate(features, support, y_val, val_mask, placeholders)\n",
    "    cost_val.append(cost)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: cost= 0.93349 accuracy= 0.58299 time= 1.28519\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_cost, test_acc, test_duration = evaluate(features, support, y_test, test_mask, placeholders)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
